Slide 1:
Hello everyone, welcome to our lecture on Scheduling, which is the seventh topic in our Operating System series, Three Easy Pieces. Today, we're going to dive into the world of scheduling, but before we do that, let's take a step back and remind ourselves what an Operating System is all about. In this lecture, we'll be exploring the key concepts that make up an OS, and I'm excited to share my knowledge with you. So, let's get started by taking a look at the fundamental components of an Operating System, which we'll cover in more detail on the next slide.
----------------
Slide 2:
Let's dive into the fundamental components of an Operating System. As we can see on this slide, what constitutes an OS is essentially the kernel plus other components.

The kernel is a privileged process that runs in kernel mode, which is controlled by the hardware. It's loaded by a bootloader, such as GRUB or a custom one, at boot time and will continue to run until the device is turned off.

Let's break down some key points here. The kernel enables a user process to run on the CPU, but in a limited direct execution fashion, and this is achieved through a context switch. Think of a context switch like a referee in a game, switching between different players, or in this case, processes, to ensure each one gets a turn to use the CPU.

The kernel also performs privileged operations on behalf of user processes, and this is done through system calls, which are invoked via a trap instruction. This means that when a user process needs to do something that requires special privileges, like accessing a hardware device, it sends a request to the kernel, which then performs the operation on its behalf.

As we understand the role of the kernel and how it interacts with user processes, we can start to think about how the OS manages the execution of these processes. What happens when we have multiple processes competing for the CPU's attention, and how do we decide which one to run next!
----------------
Slide 3:
Let's take a look at this slide on developing a scheduling policy. As we've been discussing the role of the operating system in managing the CPU, we now need to consider how the OS decides which user process to run on the CPU.

If we look at the points here, we have How do we select which user process to run on the CPU. This is essentially the core question of scheduling. The operating system has to make a decision about which process to give the CPU to, and this decision is crucial for the overall performance of the system.

Next, we have What are key assumptions. This refers to the assumptions we make about the workload, the characteristics of the processes, and the system's resources. For example, do we assume that the processes are CPU-bound or I/O-bound. Do we assume that the system has a certain amount of memory or a certain number of CPUs. These assumptions can significantly impact our scheduling policy.

We also need to consider What metrics are important. This could include metrics such as throughput, response time, turnaround time, or fairness. Different metrics may be important in different scenarios, and our scheduling policy should be designed to optimize the metrics that matter most.

Finally, we have What are some historical approaches. This refers to the various scheduling algorithms that have been developed over time, such as First-Come-First-Served, Shortest Job First, Priority Scheduling, and Round Robin. Each of these algorithms has its strengths and weaknesses, and understanding them can help us design more effective scheduling policies.

As we conclude this lesson on scheduling, I'd like to leave you with a question to think about: how would you design a scheduling policy for a system with multiple CPUs and a mix of CPU-bound and I/O-bound processes. This is the last slide, and I hope this lesson has provided a good introduction to the basics of operating systems and scheduling.
----------------
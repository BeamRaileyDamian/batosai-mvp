Slide 1:
Hello everyone, welcome to today's lecture on Operating System: Three Easy Pieces. We're now on section 4, The Abstraction: The Process. In this section, we'll be exploring how operating systems create an environment that allows multiple programs to run smoothly, even when resources are limited. Think of it like a juggler keeping multiple balls in the air - the OS has to juggle multiple programs to create the illusion of limitless resources. Let's dive in and see how this works. Our first topic is going to be CPU virtualization, so let's take a look at how the OS can create the illusion of many CPUs when we only have a few. Next, we'll explore how this is achieved through time-sharing and context-switching, so let's move on to the next slide to find out.
----------------
Slide 2:
Let's take a look at how to provide the illusion of many CPUs. As we can see from the slide, users want to run many programs, but we're limited by the number of CPUs we have, for example, 8 cores. To address this, the OS uses a technique called CPU virtualization.

The OS can promote the illusion that many virtual CPUs exist by juggling programs. This is achieved through time-sharing, where the OS runs one process, then stops it and runs another. This process is repeated, creating the illusion that multiple CPUs are available.

For instance, let's say we have two programs, A and B. The OS will run program A for a short period, then stop it and run program B. This switching between programs happens quickly, creating the illusion that both programs are running simultaneously.

However, this time-sharing approach comes with a potential cost, performance. The OS achieves this time-sharing through a mechanism called context-switch, which involves switching between the states of different processes.

Now, let's think about how these mechanisms work together to enable the OS to make decisions about which processes to run and when, which leads us to our next topic.
----------------
Slide 3:
Let's take a look at the current slide, which is about mechanisms and policies. As we revisit these concepts, we can see that there are two key points to consider: mechanisms and policies.

A mechanism refers to low-level methods or protocols that implement a functionality. In other words, its about the how - how something is done. For example, context-switching is a mechanism that allows the operating system to switch between different processes.

On the other hand, a policy refers to algorithms for making certain kinds of decisions within the operating system. This is more about the which - which process to run, which resource to allocate, and so on. A classic example of a policy is the scheduling policy, which decides which program to run first, and which one to run next.

To illustrate the difference, think of a mechanism as the steps you take to get from one place to another, while a policy is the decision of where to go and when. The operating system uses mechanisms to implement policies, and understanding this distinction is crucial in designing and implementing efficient operating systems.

As we explore these concepts further, well see how they relate to the idea of a process, which is the next topic well be discussing - what exactly constitutes a process, and how does the operating system manage it
----------------
Slide 4:
Let's take a look at the current slide, which is about a process. Mechanisms and policies are crucial in operating systems, and now we're going to dive into what exactly constitutes a process.

A program itself is lifeless – it just sits on the disk as a bunch of instructions and some data, also known as a program image. To characterize a process, we need to look at the different data structures and system resources it uses or accesses.

What comprises a process. It's made up of several components, including the machine state. The machine state is composed of memory, which is the address space – a range of addresses that a process can access. This includes instructions or code, as well as data.

Another component is registers, which are high-speed storage for small data items. We have the program counter, also known as the instruction pointer, which is represented by RIP. We also have the stack pointer, represented by RSP, and the frame or base pointer, represented by RBP.

A process also includes I/O information, which is a list of open files.

A process is a running program, and it's made up of these different components that work together to execute instructions and perform tasks.

How do we interact with these processes, and what kind of APIs are available to manage them!
----------------
Slide 5:
Let's take a look at the current slide, which is about the Process API. A process is a running program, and it's made up of several components that work together to execute instructions and perform tasks. Now, we want to interact with these processes, and to do that, we need to use certain APIs, or Application Programming Interfaces.

These APIs are available on any modern operating system, and they allow us to manage processes in various ways. We have Create, which allows us to create a new process to run a program. This can be done using functions such as fork, exec, and clone.

Next, we have Destroy, which enables us to halt a runaway process. This can be achieved by using the kill function, specifically kill pid SIGTERM, where pid is the process ID and SIGTERM is the signal sent to terminate the process.

Then, there's Wait, which allows us to wait for a process to stop running. We can use the wait function, specifically wait pid, to achieve this.

We also have Miscellaneous Control, which provides methods to suspend a process and then resume it. This can be done using the kill function with different signals, such as kill pid SIGSTOP to suspend and kill pid SIGCONT to resume.

Lastly, we have Status, which enables us to get some status information about a process. We can use the cat proc pid status command to achieve this.

Now that we've covered the different APIs available for managing processes, let's move on to the next step: how do we actually create a new process, and what are the steps involved in this process?
----------------
Slide 6:
Let's take a look at the current slide, which is about the process creation. As we discussed earlier, we have the Process API that allows us to manage processes, and now we're going to dive into the details of how a new process is created.

The first step in process creation is to load a program code and static data into memory, into the address space of the process. This means that the operating system takes the program code and data from disk, where it's stored in an executable format like ELF or PE, and loads it into the process's memory space. The key thing to note here is that the OS performs this loading process lazily, which means it only loads the pieces of code or data that are needed during program execution. This is especially useful for large programs that may not need all of their code or data at the same time.

The next step is to allocate the program's run-time stack. The stack is used for local variables, function parameters, and return addresses, so it's an essential part of the program's execution. The stack is initialized with the arguments passed to the main function, which are typically represented by the argc and argv arrays. This sets the stage for the program to start executing.

Now, let's see how the rest of the process creation unfolds on the next slide.
----------------
Slide 7:
Let's take a look at the current slide, which continues our discussion on process creation. As we've already covered the initial steps of loading the program code and static data into memory, as well as allocating the program's run-time stack, we'll now dive into the next stages of this process.

According to point 3 on the slide, The program’s heap is created usually done lazily. The heap is essentially a region of memory where the program can dynamically allocate memory for its data. This is useful when the program needs to request memory at runtime, rather than having it predefined at compile time. The program can request this space by calling malloc and free it by calling free. Think of it like a dynamic storage space that the program can use as needed.

Moving on to point 4, The OS does some other initialization tasks. This includes setting up input/output or I/O operations. By default, each process has three open file descriptors: standard input, standard output, and standard error, which are represented by the numbers 0, 1, and 2, respectively. These file descriptors are the channels through which the process can interact with the outside world, such as reading input from the keyboard or writing output to the screen.

Finally, point 5 states, Start the program running at the entry point, namely main. This is where the program actually begins execution. The operating system transfers control of the CPU to the newly-created process, allowing it to start running its code. This is the point where the program takes over and starts executing its instructions, using the resources and memory that have been allocated to it.

Now that we've covered the key stages of process creation, let's see how the program's memory is organized and managed, as we transition to the next slide on loading a program into a process.
----------------
Slide 8:
Let's take a look at the current slide, which is titled Loading From Program To Process. As we can see, it outlines the key components involved in loading a program into a process. We have the program on disk, which consists of code and static data, and the process, which has its own memory space comprising code, static data, a heap, and a stack.

The loading process takes the on-disk program and reads it into the address space of the process. This is a crucial step in creating a new process, as it allows the program to be executed by the CPU. The CPU can then fetch instructions from the process's memory space and execute them.

To break it down further, let's consider the components involved. The code segment contains the program's instructions, while the static data segment contains initialized data that the program uses. The heap is a region of memory where the program can dynamically allocate memory for its data, and the stack is used to store function calls and local variables.

When the program is loaded into memory, the operating system sets up the memory space for the process, including the code, static data, heap, and stack. The CPU can then execute the instructions in the code segment, using the data in the static data segment and allocating memory on the heap as needed.

As we can see from the slide, the loading process is a critical step in creating a new process, and it's essential to understand how the program's memory is organized and managed. Now, what happens to a process once it's loaded and running, and how does the operating system manage its state?
----------------
Slide 9:
Let's take a look at the current slide, which is titled Process States. As we can see, it outlines the different states a process can be in. According to this slide, A process can be in one of three states. These states are Running, Ready, and Blocked.

When a process is Running, it means that the process is currently being executed by the processor. The processor is fetching instructions from the process's memory space and executing them. For example, if a process is performing a calculation, it would be in the Running state while the calculation is being executed.

On the other hand, when a process is Ready, it means that the process is ready to run, but for some reason, the operating system has chosen not to run it at this given moment. This could be because the processor is currently busy executing another process, or because the operating system has decided to prioritize another process.

A process can also be in a Blocked state, which means that the process has performed some kind of operation and is waiting for the result. For instance, if a process initiates an I/O request to a disk, it becomes blocked and waits for the disk to respond. During this time, the processor can execute another process, which is more efficient than having the processor wait for the disk to respond.

Additionally, the slide mentions that Additional states may be present depending on the OS, such as Initial and Final/Terminated states. These states may vary depending on the specific operating system being used.

Now, let's think about how these process states change over time - how does a process transition from one state to another, and what factors influence these transitions?
----------------
Slide 10:
Let's take a look at the current slide, which is titled Process State Transitions. As we can see, it outlines how a process can transition from one state to another. According to this slide, the transitions are decided by the scheduler module of the OS.

The scheduler module is essentially the part of the operating system that decides which process to run next. It's like a manager that allocates the processor's time to different processes.

Now, let's go through the transitions. We have Descheduled and Scheduled. When a process is descheduled, it means the scheduler has decided to stop the process from running, and when it's scheduled, the scheduler has decided to run the process.

We also have Ready and Blocked states. As we discussed earlier, a process is in the Ready state when it's ready to run but the scheduler hasn't allocated the processor to it yet. A process is in the Blocked state when it's waiting for some operation to complete, like an I/O request.

For example, if a process initiates an I/O request to a disk, it becomes blocked and waits for the disk to respond. During this time, the processor can execute another process, which is more efficient than having the processor wait for the disk to respond.

The transition from Ready to Running is initiated by the scheduler, and the transition from Running to Blocked is initiated by the process itself when it performs an I/O operation. The transition from Blocked to Ready occurs when the I/O operation is completed.

Now, let's think about how these transitions work in a real-world scenario, and we'll explore this further in the next slide, Tracing Process States CPU Only.
----------------
Slide 11:
Let's take a look at the current slide, which is titled Tracing Process States CPU Only. As we can see, it outlines the execution of two processes, Process 0 and Process 1, over time, assuming a single processor is available.

The table on this slide shows us the state of each process at different times. We have the Time column, the Process 0 column, the Process 1 column, and some Notes. Let's go through this step by step. At time 1, Process 0 is Running, and Process 1 is Ready. This makes sense because, with only one processor, Process 1 has to wait for its turn to run.

As we move to times 2 and 3, Process 0 is still Running, and Process 1 remains Ready. At time 4, Process 0 is still Running, but now it's noted that Process 0 is now done. This means it has completed its task and is terminated.

After Process 0 is done, at times 5, 6, 7, and 8, Process 1 gets its turn to run. It's Running during these times, and finally, at time 8, it's noted that Process 1 is now done as well.

This example helps us understand how process states change over time when we only have CPU operations to consider. Now, let's think about how these process states might change if we introduce I/O operations into the mix, which we'll explore in the next slide, Tracing Process States CPU and I/O.
----------------
Slide 12:
Let's take a look at the current slide, which is titled Tracing Process States CPU and I/O. As we can see, it outlines the execution of two processes, Process 0 and Process 1, over time, assuming a single processor is available.

The table on this slide shows us the state of each process at different times. We have the Time column, the Process 0 column, the Process 1 column, and some Notes. Let's go through this step by step. At time 1, Process 0 is Running, and Process 1 is Ready. This makes sense because, with only one processor, Process 1 has to wait for its turn to run.

As we move to times 2 and 3, Process 0 is still Running, and Process 1 remains Ready. At time 4, Process 0 initiates an I/O operation, which means it needs to perform some input/output task, such as reading from a disk or writing to a file. As a result, Process 0 becomes Blocked, meaning it's waiting for the I/O operation to complete.

At time 5, Process 0 is Blocked, and Process 1 is now Running, taking advantage of the available processor. Process 1 continues to run at times 6.

At time 7, the I/O operation for Process 0 is complete, so its state changes to Ready. However, since Process 1 is still running, Process 0 has to wait.

Finally, at time 9, Process 1 is done, and Process 0 gets its turn to run again, and it's Running.

This example helps us understand how process states change over time when we have both CPU and I/O operations to consider. Now, let's think about how the operating system keeps track of all these processes and their states, which is what we'll explore in the next slide on data structures.
----------------
Slide 13:
Let's take a look at the current slide, which is titled Data Structures. As we can see, it outlines the key data structures that the operating system uses to track various pieces of information.

The OS has some key data structures that track various relevant pieces of information. We have Process/Tasks lists, which include Ready processes, Blocked processes, and the Currently running process. These lists are essential for the operating system to manage the different processes and their states.

We also have the Register context, which will hold the values of the registers when a process is stopped or paused. This is done by copying the register values to a data structure in memory. And when the process is resumed, these values will be copied back to the actual registers during a context-switch. This is crucial for the operating system to save and restore the state of a process.

Another important data structure is the Process Control Block or Task Structure in Linux. This is a C-structure that contains information about a process. Its like a container that holds all the relevant details about a process, making it easier for the operating system to manage and keep track of the process.

Now, let's think about how these data structures are actually implemented in a real operating system, which is what we'll explore in the next slide.
----------------
Slide 14:
Let's take a look at the current slide, which outlines the xv6 kernel's register context structure and process states definitions. As we can see, it defines a struct called context which holds the values of the registers when a process is stopped or paused.

The context struct includes several key registers: eip, the instruction pointer register, esp, the stack pointer register, ebx, ecx, edx, esi, and edi, which are general-purpose registers, and ebp, the stack base pointer register. These registers are crucial for the operating system to save and restore the state of a process.

In addition to the context struct, we also have an enum called proc_state that defines the different states a process can be in. These states include UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, and ZOMBIE. Each of these states represents a specific point in a process's lifecycle, from being unused to being currently running.

To illustrate this, think of a process as a person working on a task. When the person is not working, they are like a process in the UNUSED state. As they start working, they become RUNNABLE, and when they are actually doing the task, they are RUNNING. If they need to wait for something, they become SLEEPING, and if they are waiting for a resource, they might be in the ZOMBIE state.

Now, let's think about how these data structures are used in the actual implementation of the xv6 kernel, specifically in the process control block, which we'll explore in the next slide.
----------------
Slide 15:
Let's examine the current slide, which displays the xv6 kernel proc structure definition. This structure contains information about each process, including its register context and state. The struct proc has several key fields: mem and sz (process memory start and size), kstack (kernel stack pointer), state (current process state), pid (process ID), and parent (parent process pointer).

Other fields include chan (indicating if the process is sleeping on a channel), killed (indicating process termination), ofile (open files), cwd (current directory), context (register context), and tf (trap frame for the current interrupt). These fields are crucial for managing processes and implementing system calls.

When a process is created, the operating system allocates memory, initializes its register context, and tracks its state. The pid field uniquely identifies each process, while the parent field establishes relationships between processes. The chan and killed fields manage process synchronization and termination.

In summary, the struct proc is a vital data structure in the xv6 kernel, providing a comprehensive representation of a process. Understanding its fields is essential for managing processes and implementing process-related system calls. With this knowledge, you now have a better understanding of how operating systems manage processes and implement related functionality.
----------------